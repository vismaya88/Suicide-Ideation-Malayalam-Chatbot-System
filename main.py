import pandas as pd
import torch
from transformers import MT5Tokenizer, MT5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq
from datasets import Dataset
import gradio as gr
import os
import re
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics.pairwise import cosine_similarity
import joblib
from pathlib import Path
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import numpy as np

severity_model = None
severity_tokenizer = None
severity_label_encoder = None

tokenizer = None
model = None



# Dictionary of default responses based on keyword severity
default_responses = {
    "anxiety": "‡¥§‡¥æ‡¥ô‡µç‡¥ï‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Ü‡¥∂‡¥ô‡µç‡¥ï ‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥≤‡¥æ‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ. ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥Ü‡¥¥‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥∂‡µç‡¥µ‡¥∏‡¥ø‡¥ö‡µç‡¥ö‡µç ‡¥∂‡¥∞‡µÄ‡¥∞‡¥Ç ‡¥∂‡¥æ‡¥®‡µç‡¥§‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÇ.",
    "depression": "‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥ï‡µç‡¥∑‡µÄ‡¥£‡¥Ç ‡¥Ö‡¥§‡¥ø‡¥∞‡µÅ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥®‡µç‡¥®‡µÅ. ‡¥í‡¥∞‡¥æ‡¥≥‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥∏‡¥Ç‡¥∏‡¥æ‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÇ, ‡¥í‡¥±‡µç‡¥±‡¥Ø‡¥æ‡¥£‡µÜ‡¥®‡µç‡¥®‡µç ‡¥ï‡¥∞‡µÅ‡¥§‡µá‡¥£‡µç‡¥ü.",
    "suicidal": "‡¥à ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£‡¥Ç ‡¥Ö‡¥§‡µç‡¥Ø‡¥®‡µç‡¥§‡¥Ç ‡¥ó‡µó‡¥∞‡¥µ‡¥Æ‡µá‡¥±‡¥ø‡¥Ø‡¥§‡¥æ‡¥£‡µç. ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥â‡¥ü‡µª ‡¥∏‡¥π‡¥æ‡¥Ø‡¥Ç ‡¥§‡µá‡¥ü‡µÅ‡¥ï. ‡¥§‡¥æ‡¥ô‡µç‡¥ï‡µæ ‡¥§‡¥®‡¥ø‡¥ö‡µç‡¥ö‡¥≤‡µç‡¥≤."
}

severity_keywords = {
    "anxiety": ["‡¥â‡¥±‡¥ï‡µç‡¥ï‡¥Ç", "‡¥ö‡¥ø‡¥®‡µç‡¥§", "‡¥Ö‡¥∂‡¥æ‡¥®‡µç‡¥§‡¥§", "‡¥â‡¥≥‡µÅ‡¥™‡µç‡¥™‡µç", "‡¥§‡¥≥‡µº‡¥ö‡µç‡¥ö", "‡¥®‡¥ü‡µç‡¥ü‡µÅ", "‡¥á‡¥≥‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ"],
    "depression": ["‡¥µ‡µá‡¥¶‡¥®", "‡¥á‡¥ö‡µç‡¥õ‡¥Ø‡¥ø‡¥≤‡µç‡¥≤‡¥æ‡¥Ø‡µç‡¥Æ", "‡¥®‡¥ø‡¥∞‡¥æ‡¥∂", "‡¥¶‡µÅ:‡¥ñ‡¥Ç", "‡¥§‡¥≥‡µº‡¥®‡µç‡¥®‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ", "‡¥Æ‡¥®‡µã‡¥µ‡µà‡¥ï‡¥≤‡µç‡¥Ø‡¥Ç"],
    "suicidal": ["‡¥ö‡¥æ‡¥ï‡¥£‡¥Ç", "‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Ç ‡¥µ‡µá‡¥£‡µç‡¥ü", "‡¥∏‡¥Æ‡¥æ‡¥™‡¥®‡¥Ç", "‡¥â‡¥™‡µá‡¥ï‡µç‡¥∑‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª", "‡¥Ö‡¥µ‡¥∏‡¥æ‡¥®‡¥Ç", "‡¥ú‡µÄ‡¥µ‡¥®‡µç"]
}
intent_classifier = None
vectorizer = None
sentiment_lexicon = {"‡¥â‡¥≤‡µç‡¥≤‡¥æ‡¥∏‡¥Ç": 1, "‡¥™‡µç‡¥∞‡¥ø‡¥Ø‡¥Ç": 1, "‡¥µ‡µá‡¥¶‡¥®": -1, "‡¥¶‡µÅ:‡¥ñ‡¥Ç": -1, "‡¥®‡¥ø‡¥∏‡µç‡¥∏‡¥æ‡¥∞‡¥§": -1}

embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
memory_bank = {
    "‡¥§‡¥≥‡µº‡¥ö‡µç‡¥ö": "‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Ç ‡¥µ‡µÜ‡¥±‡µÅ‡¥™‡µç‡¥™‡µÅ‡¥£‡µç‡¥ü‡µç ‡¥é‡¥®‡µç‡¥® ‡¥Ö‡¥®‡µÅ‡¥≠‡¥µ‡¥Ç ‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ. ‡¥∏‡¥π‡¥æ‡¥Ø‡¥Ç ‡¥§‡µá‡¥ü‡µÅ‡¥ï.",
    "‡¥Ö‡¥∂‡¥æ‡¥®‡µç‡¥§‡¥§": "‡¥∂‡¥æ‡¥®‡µç‡¥§‡¥Æ‡¥æ‡¥ï‡¥æ‡µª ‡¥¶‡µÄ‡µº‡¥ò‡¥∂‡µç‡¥µ‡¥æ‡¥∏‡¥Ç ‡¥é‡¥ü‡µÅ‡¥ï‡µç‡¥ï‡µÅ‡¥ï. ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥á‡¥§‡¥ø‡µΩ ‡¥®‡¥ø‡¥®‡µç‡¥®‡µç ‡¥Æ‡¥æ‡¥±‡¥æ‡µª ‡¥ï‡¥¥‡¥ø‡¥Ø‡µÅ‡¥Ç.",
    "‡¥ö‡¥æ‡¥ï‡¥£‡¥Ç": "‡¥§‡¥æ‡¥ô‡µç‡¥ï‡¥≥‡µÅ‡¥ü‡µÜ ‡¥ú‡µÄ‡¥µ‡µª ‡¥Ö‡¥§‡µç‡¥Ø‡¥®‡µç‡¥§‡¥Ç ‡¥µ‡¥ø‡¥≤‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü‡¥§‡¥æ‡¥£‡µç. ‡¥â‡¥ü‡µª ‡¥∏‡¥π‡¥æ‡¥Ø‡¥Ç ‡¥§‡µá‡¥ü‡µÅ‡¥ï."
}
df = pd.read_excel(r"C:\Users\Devika\Desktop\Study\Sem6\NLP\Project\Suicide_detection_chatbot\therapy_malayalam_cleaned_utf8.xlsx")
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['inputs'].astype(str))

def load_dataset(file_path):
    df = pd.read_excel(file_path)
    df = df.rename(columns={"inputs": "input_text", "targets": "target_text"})
    return Dataset.from_pandas(df[["input_text", "target_text"]])

def tokenize_function(example):
    model_inputs = tokenizer(example["input_text"], max_length=512, padding="max_length", truncation=True)
    labels = tokenizer(example["target_text"], max_length=128, padding="max_length", truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

def train_model(file_path):
    print("üîÑ Loading data...")
    dataset = load_dataset(file_path)
    tokenized_dataset = dataset.map(tokenize_function)
    print("üß† Starting training...")
    training_args = TrainingArguments(
        output_dir="./mt5_malayalam_chatbot",
        per_device_train_batch_size=4,
        num_train_epochs=3,
        save_strategy="epoch",
        logging_steps=10,
        fp16=torch.cuda.is_available(),
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
        data_collator=DataCollatorForSeq2Seq(tokenizer, model)
    )
    trainer.train()

    trainer.save_model(r"C:\Users\Devika\Desktop\Study\Sem6\NLP\Project\Suicide_detection_chatbot\mt5_malayalam_chatbot")
    tokenizer.save_pretrained(r"C:\Users\Devika\Desktop\Study\Sem6\NLP\Project\Suicide_detection_chatbot\mt5_malayalam_chatbot")
    print("‚úÖ Training complete and model saved.")


def get_similar_response(text):
    embeddings = embedding_model.encode([text] + list(memory_bank.keys()), convert_to_tensor=True)
    cosine_scores = util.pytorch_cos_sim(embeddings[0], embeddings[1:])
    best_idx = torch.argmax(cosine_scores).item()
    if cosine_scores[0][best_idx] > 0.7:
        return list(memory_bank.values())[best_idx]
    return None

def load_model():
    global tokenizer, model,  vectorizer
    if os.path.exists(r"C:\Users\Devika\Desktop\Study\Sem6\NLP\Project\Suicide_detection_chatbot\mt5_malayalam_chatbot"):
        model = MT5ForConditionalGeneration.from_pretrained(r"C:\Users\Devika\Desktop\Study\Sem6\NLP\Project\Suicide_detection_chatbot\mt5_malayalam_chatbot")
        tokenizer = MT5Tokenizer.from_pretrained(r"C:\Users\Devika\Desktop\Study\Sem6\NLP\Project\Suicide_detection_chatbot\mt5_malayalam_chatbot")
    else:
        model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
        tokenizer = MT5Tokenizer.from_pretrained("google/mt5-small")
   
    
        




# Sample phrases representing each severity class
severity_samples = {
    "DEPRESSION": [
        "‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥í‡¥®‡µç‡¥®‡µÅ‡¥Ç ‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥≤‡¥æ‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥ø‡¥≤‡µç‡¥≤",
        "‡¥∏‡¥π‡¥æ‡¥Ø‡¥Ç ‡¥µ‡µá‡¥£‡¥Ç",
        "‡¥¶‡µÅ:‡¥ñ‡¥Ç ‡¥â‡¥£‡µç‡¥ü‡µç",
        "‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Ç ‡¥¨‡µÅ‡¥¶‡µç‡¥ß‡¥ø‡¥Æ‡µÅ‡¥ü‡µç‡¥ü‡¥ø‡¥≤‡¥æ‡¥£‡µç",
        "‡¥û‡¥æ‡¥®‡µç‚Äç ‡¥§‡¥≥‡¥∞‡µç‚Äç‡¥®‡µç‡¥®‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ"
    ],
    "ANXIETY": [
        "‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥™‡µá‡¥ü‡¥ø‡¥Ø‡¥æ‡¥£‡µç",
        "‡¥û‡¥æ‡¥®‡µç‚Äç ‡¥Ö‡¥≤‡µã‡¥∏‡¥∞‡¥§‡µç‡¥§‡¥ø‡¥≤‡¥æ‡¥£‡µç",
        "‡¥Ö‡¥µ‡¥∏‡¥æ‡¥®‡¥Ç ‡¥é‡¥®‡µç‡¥§‡¥æ‡¥µ‡µÅ‡¥Ç ‡¥é‡¥®‡µç‡¥®‡µç ‡¥≠‡¥Ø‡¥Æ‡¥æ‡¥£‡µç",
        "‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡µç ‡¥∂‡¥æ‡¥®‡µç‡¥§‡¥Æ‡¥≤‡µç‡¥≤",
        "‡¥µ‡¥ø‡¥ï‡¥æ‡¥∞‡¥™‡¥∞‡¥Æ‡¥æ‡¥Ø ‡¥â‡¥≥‡¥û‡µç‡¥û‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ"
    ],
    "SUICIDAL": [
        "‡¥û‡¥æ‡¥®‡µç‚Äç ‡¥ú‡µÄ‡¥µ‡¥®‡µÜ‡¥ü‡µÅ‡¥§‡µç‡¥§‡µá‡¥ï‡µç‡¥ï‡µÅ‡¥Ç",
        "‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Ç ‡¥Ö‡¥µ‡¥∏‡¥æ‡¥®‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç",
        "‡¥û‡¥æ‡¥®‡µç‚Äç ‡¥Æ‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç",
        "‡¥é‡¥®‡µç‡¥§‡¥ø‡¥®‡µç ‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥ú‡µÄ‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç",
        "‡¥á‡¥§‡µç ‡¥é‡¥®‡µç‡¥®‡µÜ ‡¥ï‡µä‡¥≤‡µç‡¥≤‡µÅ‡¥®‡µç‡¥®‡µÅ"
    ],
    "GENERAL": [
        "‡¥∏‡¥æ‡¥ß‡¥æ‡¥∞‡¥£‡¥Æ‡¥æ‡¥Ø ‡¥¶‡¥ø‡¥µ‡¥∏‡¥Æ‡¥æ‡¥£‡µç",
        "‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥ö‡µÜ‡¥±‡¥ø‡¥Ø ‡¥µ‡¥ø‡¥∑‡¥Æ‡¥Ç ‡¥â‡¥£‡µç‡¥ü‡µç",
        "‡¥í‡¥®‡µç‡¥®‡µÅ‡¥Ç ‡¥µ‡¥≤‡¥ø‡¥Ø ‡¥™‡µç‡¥∞‡¥∂‡µç‡¥®‡¥Æ‡¥ø‡¥≤‡µç‡¥≤"
    ]
}

# Function to compute severity based on meaning
def predict_severity_semantic(text):
    input_embedding = embedding_model.encode(text, convert_to_tensor=True)

    max_score = -1
    predicted_severity = "GENERAL"

    for severity, phrases in severity_samples.items():
        for phrase in phrases:
            phrase_embedding = embedding_model.encode(phrase, convert_to_tensor=True)
            similarity = util.cos_sim(input_embedding, phrase_embedding).item()

            if similarity > max_score:
                max_score = similarity
                predicted_severity = severity

    return predicted_severity





def detect_severity(text):
    suicidal_keywords = ['‡¥Ü‡¥§‡µç‡¥Æ‡¥π‡¥§‡µç‡¥Ø', '‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Ç ‡¥Ö‡¥µ‡¥∏‡¥æ‡¥®‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç', '‡¥ö‡¥§‡µç‡¥§‡µÅ‡¥ï‡¥≥‡¥Ø‡¥£‡¥Ç', '‡¥ú‡µÄ‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡¥æ‡¥ï‡¥ø‡¥≤‡µç‡¥≤']
    depression_keywords = ['‡¥â‡¥≥‡¥≥‡¥ø‡¥≤‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï', '‡¥¶‡µÅ‡¥É‡¥ñ‡¥Ç', '‡¥ö‡¥ø‡¥®‡µç‡¥§', '‡¥â‡¥≥‡¥≥‡¥æ‡¥∂‡¥Ø','‡¥∏‡¥π‡¥æ‡¥Ø‡¥Ç ‡¥µ‡µá‡¥£‡¥Ç']
    anxiety_keywords = ['‡¥Ö‡¥∂‡¥æ‡¥®‡µç‡¥§‡¥ø', '‡¥≠‡¥Ø‡¥Ç', '‡¥™‡¥æ‡¥ï‡¥Ç', '‡¥â‡¥§‡µç‡¥ï‡¥£‡µç‡¥†', '‡¥™‡µá‡¥ü‡¥ø']
    if any(kw in text for kw in suicidal_keywords):
        return "SUICIDAL"
    elif any(kw in text for kw in depression_keywords):
        return "DEPRESSION"
    elif any(kw in text for kw in anxiety_keywords):
        return "ANXIETY"
    else:
        return "GENERAL"
    
def generate_response(user_input):
    user_vec = vectorizer.transform([user_input])
    similarity_scores = cosine_similarity(user_vec, X)
    best_match_idx = similarity_scores.argmax()
    response = df.iloc[best_match_idx]['targets']

    
    severity = predict_severity_semantic(user_input)
    rule_severity = detect_severity(user_input)
    


    print(f"Severity: {rule_severity}")
    print(f"Response: {response}")

    return f"Severity (Rule): {rule_severity}\nSeverity (Model): {severity}\n\nResponse: {response}"









def run_gradio():
    iface = gr.Interface(
        fn=generate_response,
        inputs=gr.Textbox(label=" Malayalam ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Ç ‡¥®‡µΩ‡¥ï‡µÅ‡¥ï"),
        outputs=gr.Textbox(label="‡¥ö‡¥æ‡¥±‡µç‡¥±‡µç‡¥¨‡µã‡¥ü‡µç‡¥ü‡µç ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£‡¥µ‡µÅ‡¥Ç ‡¥ó‡µÅ‡¥∞‡µÅ‡¥§‡¥∞‡¥Ç ‡¥µ‡¥ø‡¥≤‡¥Ø‡¥ø‡¥∞‡µÅ‡¥§‡µç‡¥§‡¥≤‡µÅ‡¥Ç"),
        title="Malayalam Therapy Chatbot",
        description="‡¥∏‡µó‡¥Æ‡µç‡¥Ø‡¥Æ‡¥æ‡¥Ø ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£‡¥Ç ‡¥®‡µΩ‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç ‡¥∏‡µà‡¥ï‡µç‡¥ï‡µã‡¥≥‡¥ú‡¥ø‡¥ï‡µç‡¥ï‡µΩ ‡¥∏‡¥π‡¥æ‡¥Ø‡¥ø"
        
    )
    
    iface.launch()

if __name__ == "__main__":
    load_model()
    # Uncomment below to train once
    # train_model("therapy_malayalam_cleaned_utf8.xlsx")
    
    

    
    run_gradio()
